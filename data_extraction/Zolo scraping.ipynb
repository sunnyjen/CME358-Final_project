{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "urls = [\"https://www.zolo.ca/toronto-real-estate\"] + \\\n",
    "        [\"https://www.zolo.ca/toronto-real-estate/page-\" + str(i) for i in range(2, 62)]\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "for url in urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait until a card listings are loaded.\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//ul[@class='card-listing--values truncate list-unstyled xs-flex-order-1 xs-mb05']\"))\n",
    "        )\n",
    "        print(\"Page fully loaded.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error waiting for page to load:\", e)\n",
    "\n",
    "    # Get html source\n",
    "    main_page_source = driver.page_source\n",
    "\n",
    "    # Beautiful-soup the HTML content\n",
    "    soup = BeautifulSoup(main_page_source, 'html.parser')\n",
    "\n",
    "    # Find all listing cards in the main page --> have tags named \"article\"\n",
    "    listings = soup.find_all('article')\n",
    "\n",
    "    highlight_data = []\n",
    "\n",
    "    # Loop through each listing on each page/url\n",
    "    for listing in listings:\n",
    "        \n",
    "        room_name = None\n",
    "        room_dimension = None\n",
    "        room_property = None\n",
    "\n",
    "        room_names = []\n",
    "        room_dimensions = []\n",
    "        room_properties = []\n",
    "\n",
    "        # Find all tags with href attributes --> will provide link to each listing on current page.\n",
    "        links = listing.find_all(\"a\", href=True)\n",
    "\n",
    "        # To avoid errors when href tag is actually not for the link but for another html element\n",
    "        valid_links = [link['href'] for link in links if 'https://www.zolo.ca/' in link['href']]\n",
    "\n",
    "        if valid_links:\n",
    "            # Extract from list (list will only have one element, the link)\n",
    "            sub_url = valid_links[0]\n",
    "\n",
    "            # Navigate to the sub URL\n",
    "            driver.get(sub_url)\n",
    "\n",
    "        \n",
    "            try:\n",
    "            # Wait for the element to be present if it exists\n",
    "                WebDriverWait(driver, 40).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//dd[@class='column-value']\"))\n",
    "                )\n",
    "            except:\n",
    "                # If the <dd> class_='column-value' is not found within the timeout\n",
    "                print(\"Element not found within the given time.\")\n",
    "\n",
    "            # Get page source (HTML)\n",
    "            sub_page_source = driver.page_source\n",
    "\n",
    "            # Beautiful-soup the HTML content of the subpage\n",
    "            sub_soup = BeautifulSoup(sub_page_source, 'html.parser')\n",
    "\n",
    "            highlight_info = {}\n",
    "            \n",
    "            # This is for the info above the description. They are all under \"dt\" tags\n",
    "            high_level_labels = sub_soup.find_all(\"dt\", class_=\"column-label\")\n",
    "            \n",
    "            for label in high_level_labels:\n",
    "            # The value is the next \"dd\" tag's \"span\" text (usually)\n",
    "                try:\n",
    "                    value = label.find_next(\"dd\", class_=\"column-value\").find(\"span\", class_=\"priv\")\n",
    "        \n",
    "                    # Get the text content for label and value if they exist\n",
    "                    label_text = label.text.strip()\n",
    "                    value_text = value.text.strip() if value else None\n",
    "        \n",
    "                    if label_text and value_text:\n",
    "                        highlight_info[label_text] = value_text\n",
    "                    else:\n",
    "                    # For missing values\n",
    "                        highlight_info[label_text] = None\n",
    "                        \n",
    "                except AttributeError:\n",
    "                    # Handle the case where there might be no corresponding dd or span\n",
    "                    print(f\"Skipping label {label.text.strip()} due to missing data.\")\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # Entries after the description.\n",
    "            labels = sub_soup.find_all(\"div\", class_=\"column-label\")\n",
    "            values = sub_soup.find_all(\"div\", class_=\"column-value\")\n",
    "        \n",
    "            highlight_info = {}\n",
    "            for label, value in zip(labels, values):\n",
    "                label_text = label.text.strip()\n",
    "                value_text = value.text.strip()\n",
    "                highlight_info[label_text] = value_text\n",
    "\n",
    "            # More listing info\n",
    "            price = sub_soup.find('div', class_=\"xs-text-2 heavy xs-inline xs-mr1\").text.strip() if sub_soup.find('div', class_=\"xs-text-2 heavy xs-inline xs-mr1\") else None\n",
    "            address = sub_soup.find('h1', class_=\"address xs-text-4 sm-text-3 truncate heavy\").text.strip() if sub_soup.find('h1', class_=\"address xs-text-4 sm-text-3 truncate heavy\") else None\n",
    "\n",
    "            # Extract room information\n",
    "            rooms = sub_soup.find_all(\"section\", class_=\"listing-rooms tables md-mb5\")\n",
    "            print(rooms)\n",
    "\n",
    "            for room_section in rooms:\n",
    "                rows = room_section.find_all(\"tr\")\n",
    "\n",
    "                # Iterate through each row and extract room name, dimension, and properties\n",
    "\n",
    "                for row in rows:\n",
    "                    print(row)\n",
    "\n",
    "                    room_data = row.find_all(\"span\", class_=\"priv\")\n",
    "\n",
    "                    if len(room_data) >= 2:\n",
    "                        room_name = room_data[0].text.strip()  # First span for the room name\n",
    "                        room_dimension = room_data[1].text.strip()  # Second span for the room dimension\n",
    "\n",
    "                        # If there's a third span, it will be for the room's properties\n",
    "                        room_property = room_data[2].text.strip() if len(room_data) > 2 else None\n",
    "\n",
    "                        print(f\"Room Name: {room_name}\")\n",
    "                        print(f\"Room Dimension: {room_dimension}\")\n",
    "                        print(f\"Room Properties: {room_property}\")\n",
    "\n",
    "                        room_names.append(room_name)\n",
    "                        room_dimensions.append(room_dimension)\n",
    "                        room_properties.append(room_property)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"Not enough data in row.\")\n",
    "\n",
    "\n",
    "            listing_info = {\n",
    "                'price': price,\n",
    "                'address': address,\n",
    "                \"rooms\": room_names,\n",
    "                \"room dimensions\": room_dimensions,\n",
    "                \"room_properties\": room_properties\n",
    "            }\n",
    "\n",
    "            # Merge listing info with highlight info\n",
    "            listing_info.update(highlight_info)\n",
    "\n",
    "            # Append to highlight_data list\n",
    "            highlight_data.append(listing_info)\n",
    "        else:\n",
    "            print(\"No valid href found for this 'listing' --> skip\")\n",
    "\n",
    "    # Create a dataframe for this URL from each of the listings\n",
    "    df = pd.DataFrame(highlight_data)\n",
    "\n",
    "    # Append this dataframe to the total list of dfs from all URLs\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes for each page into a single dataframe. Keeps all columns and add NaN where necessary.\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to CSV file.\n",
    "final_df.to_csv(\"new_zolo_listings.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
